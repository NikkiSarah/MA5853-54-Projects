{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import spellchecker\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# load the language model needed later\n",
    "# this method imports and the chosen language model as a python module\n",
    "# the trf model is used over the sm model as it's more accurate (but slower)\n",
    "import en_core_web_trf\n",
    "nlp_trf = en_core_web_trf.load()\n",
    "\n",
    "# just a setting the author's system needs to plot matplotlib charts\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# load the data\n",
    "vodafone_reviews = pd.read_csv('vodafone_reviews1903.csv')\n",
    "\n",
    "### Very Preliminary EDA ###\n",
    "# define functions to classify each review into an NPS group\n",
    "def create_nps_group(row):\n",
    "    if row.score <= 3:\n",
    "        group = 'Detractor'\n",
    "    elif row.score == 4:\n",
    "        group = 'Passive'\n",
    "    else:\n",
    "        group = 'Promoter'\n",
    "    return group\n",
    "\n",
    "\n",
    "def create_nps_class(row):\n",
    "    if row.nps_group == 'Detractor':\n",
    "        nps_class = -1\n",
    "    elif row.nps_group == 'Passive':\n",
    "        nps_class = 0\n",
    "    else:\n",
    "        nps_class = 1\n",
    "    return nps_class\n",
    "\n",
    "\n",
    "vodafone_reviews['nps_group'] = vodafone_reviews.apply(create_nps_group, axis=1)\n",
    "vodafone_reviews['nps_class'] = vodafone_reviews.apply(create_nps_class, axis=1)\n",
    "\n",
    "# calculate the number of reviews by customer rating and NPS group\n",
    "reviews = pd.value_counts(vodafone_reviews.score.values).sort_index()\n",
    "reviews2 = pd.value_counts(vodafone_reviews.nps_group.values).sort_index()\n",
    "\n",
    "# plot the number of reviews by customer rating\n",
    "plt.bar(x=reviews.index, height=reviews.values, color='#990000')\n",
    "plt.xlabel('customer rating')\n",
    "plt.ylabel('number of reviews');\n",
    "\n",
    "for pos in ['right', 'top']:\n",
    "    plt.gca().spines[pos].set_visible(False)\n",
    "\n",
    "# plot the number of reviews by NPS group\n",
    "plt.bar(x=reviews2.index, height=reviews2.values, color='#990000')\n",
    "plt.xlabel('NPS group')\n",
    "plt.ylabel('number of reviews');\n",
    "\n",
    "for pos in ['right', 'top']:\n",
    "    plt.gca().spines[pos].set_visible(False)\n",
    "\n",
    "# combine the title and review columns together\n",
    "vodafone_reviews['text'] = vodafone_reviews.apply(lambda x: x.title + '. ' + x.review, axis=1)\n",
    "\n",
    "# calculate character and word lengths of the combined text\n",
    "vodafone_reviews['text_num_chars'] = vodafone_reviews.text.apply(lambda x: len(x))\n",
    "vodafone_reviews['text_num_words'] = vodafone_reviews.text.apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "# obtain some basic descriptive statistics\n",
    "descriptive_statistics = vodafone_reviews.describe()\n",
    "\n",
    "# plot the distribution of character and word lengths\n",
    "plt.hist(vodafone_reviews.text_num_chars, bins=100, edgecolor='#E60000', color='#990000')\n",
    "plt.xlabel('Number of characters')\n",
    "plt.ylabel('Number of titles');\n",
    "\n",
    "for pos in ['right', 'top']:\n",
    "    plt.gca().spines[pos].set_visible(False)\n",
    "\n",
    "plt.hist(vodafone_reviews.text_num_words, bins=25, edgecolor='#E60000', color='#990000')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of titles');\n",
    "\n",
    "for pos in ['right', 'top']:\n",
    "    plt.gca().spines[pos].set_visible(False)\n",
    "\n",
    "# define a function to display a kde plot of any possible combination of nps group members\n",
    "def plot_kde_plot(choice=1):\n",
    "    if choice == 1:\n",
    "        data = vodafone_reviews[vodafone_reviews.nps_group.isin([\"Promoter\", \"Detractor\"])]\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        p = sns.kdeplot(ax=axs[0], data=data, x='text_num_chars', hue='nps_group', fill=True,\n",
    "                        common_norm=False, palette=['#990000', '#4a4d4e'], legend=False)\n",
    "        p.spines['right'].set_visible(False)\n",
    "        p.spines['top'].set_visible(False)\n",
    "        p.set_xlabel(\"Number of characters in review\")\n",
    "        p.legend(labels=[\"Promoter\", \"Detractor\"], title = \"NPS group\")\n",
    "\n",
    "        p2 = sns.kdeplot(ax=axs[1], data=data, x='text_num_words', hue='nps_group', fill=True,\n",
    "                         common_norm=False, palette=['#990000', '#4a4d4e'], legend=False)\n",
    "        p2.spines['right'].set_visible(False)\n",
    "        p2.spines['top'].set_visible(False)\n",
    "        p2.set_xlabel(\"Number of words in review\")\n",
    "        p2.legend(labels=[\"Promoter\", \"Detractor\"], title = \"NPS group\")\n",
    "    elif choice == 2:\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        p = sns.kdeplot(ax=axs[0], data=vodafone_reviews, x='text_num_chars', hue='nps_group', fill=True,\n",
    "                        common_norm=False, palette=['#990000', '#4a4d4e', '#007c92'], legend=False)\n",
    "        p.spines['right'].set_visible(False)\n",
    "        p.spines['top'].set_visible(False)\n",
    "        p.set_xlabel(\"Number of characters in review\")\n",
    "        p.legend(labels=[\"Promoter\", \"Passive\", \"Detractor\"], title=\"NPS group\")\n",
    "\n",
    "        p2 = sns.kdeplot(ax=axs[1], data=vodafone_reviews, x='text_num_words', hue='nps_group', fill=True,\n",
    "                         common_norm=False, palette=['#990000', '#4a4d4e', '#007c92'], legend=False)\n",
    "        p2.spines['right'].set_visible(False)\n",
    "        p2.spines['top'].set_visible(False)\n",
    "        p2.set_xlabel(\"Number of words in review\")\n",
    "        p2.legend(labels=[\"Promoter\", \"Passive\", \"Detractor\"], title=\"NPS group\")\n",
    "    elif choice == 3:\n",
    "        data = vodafone_reviews[vodafone_reviews.nps_group.isin([\"Promoter\", \"Passive\"])]\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        p = sns.kdeplot(ax=axs[0], data=data, x='text_num_chars', hue='nps_group', fill=True,\n",
    "                        common_norm=False, palette=['#007c92', '#4a4d4e'], legend=False)\n",
    "        p.spines['right'].set_visible(False)\n",
    "        p.spines['top'].set_visible(False)\n",
    "        p.set_xlabel(\"Number of characters in review\")\n",
    "        p.legend(labels=[\"Promoter\", \"Passive\"], title=\"NPS group\")\n",
    "\n",
    "        p2 = sns.kdeplot(ax=axs[1], data=data, x='text_num_words', hue='nps_group', fill=True,\n",
    "                         common_norm=False, palette=['#007c92', '#4a4d4e'], legend=False)\n",
    "        p2.spines['right'].set_visible(False)\n",
    "        p2.spines['top'].set_visible(False)\n",
    "        p2.set_xlabel(\"Number of words in review\")\n",
    "        p2.legend(labels=[\"Promoter\", \"Passive\"], title=\"NPS group\")\n",
    "    elif choice == 4:\n",
    "        data = vodafone_reviews[vodafone_reviews.nps_group.isin([\"Detractor\", \"Passive\"])]\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        p = sns.kdeplot(ax=axs[0], data=data, x='text_num_chars', hue='nps_group', fill=True,\n",
    "                        common_norm=False, palette=['#990000', '#007c92'], legend=False)\n",
    "        p.spines['right'].set_visible(False)\n",
    "        p.spines['top'].set_visible(False)\n",
    "        p.set_xlabel(\"Number of characters in review\")\n",
    "        p.legend(labels=[\"Passive\", \"Detractor\"], title=\"NPS group\")\n",
    "\n",
    "        p2 = sns.kdeplot(ax=axs[1], data=data, x='text_num_words', hue='nps_group', fill=True,\n",
    "                         common_norm=False, palette=['#990000', '#007c92'], legend=False)\n",
    "        p2.spines['right'].set_visible(False)\n",
    "        p2.spines['top'].set_visible(False)\n",
    "        p2.set_xlabel(\"Number of words in review\")\n",
    "        p2.legend(labels=[\"Passive\", \"Detractor\"], title=\"NPS group\")\n",
    "    else:\n",
    "        print(\"Invalid selection.\")\n",
    "\n",
    "\n",
    "plot_kde_plot(choice=4)\n",
    "\n",
    "# an alternative representation using seaborn pointplots\n",
    "p = sns.pointplot(data=vodafone_reviews, x='score', y='text_num_chars', color='#990000', ci=None)\n",
    "p.spines['right'].set_visible(False)\n",
    "p.spines['top'].set_visible(False)\n",
    "p.set_xlabel(\"Customer rating\")\n",
    "p.set_ylabel(\"Number of characters\");\n",
    "\n",
    "p2 = sns.pointplot(data=vodafone_reviews, x='nps_group', y='text_num_chars', color='#990000', ci=None)\n",
    "p2.spines['right'].set_visible(False)\n",
    "p2.spines['top'].set_visible(False)\n",
    "p2.set_xlabel(\"NPS group\")\n",
    "p2.set_ylabel(\"Number of characters\");\n",
    "\n",
    "p3 = sns.pointplot(data=vodafone_reviews, x='score', y='text_num_words', color='#990000', ci=None)\n",
    "p3.spines['right'].set_visible(False)\n",
    "p3.spines['top'].set_visible(False)\n",
    "p3.set_xlabel(\"Customer rating\")\n",
    "p3.set_ylabel(\"Number of words\");\n",
    "\n",
    "p4 = sns.pointplot(data=vodafone_reviews, x='nps_group', y='text_num_words', color='#990000', ci=None)\n",
    "p4.spines['right'].set_visible(False)\n",
    "p4.spines['top'].set_visible(False)\n",
    "p4.set_xlabel(\"NPS group\")\n",
    "p4.set_ylabel(\"Number of words\");\n",
    "\n",
    "# calculate and plot the correlation between nps_group, customer rating, and title and review lengths\n",
    "numeric_features = vodafone_reviews.loc[:, ['score', 'nps_class', 'text_num_chars', 'text_num_words']]\n",
    "corr = numeric_features.corr()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mask = np.zeros_like(numeric_features.corr())\n",
    "mask[np.triu_indices_from(mask)] = 1\n",
    "sns.heatmap(numeric_features.corr(), mask=mask, ax=ax, annot=True, cmap='Reds')\n",
    "\n",
    "# save all objects created thus far\n",
    "def save_created_objects():\n",
    "    object_names = [reviews, reviews2, descriptive_statistics, vodafone_reviews, numeric_features]\n",
    "    reviews.to_csv('part1_reviews.csv', header=True)\n",
    "    reviews2.to_csv('part1_reviews2.csv', header=True)\n",
    "    descriptive_statistics.to_csv('part1_descriptive_statistics.csv', header=True)\n",
    "    vodafone_reviews.to_csv('part1_vodafone_reviews.csv', header=True)\n",
    "    numeric_features.to_csv('part1_numeric_features.csv', header=True)\n",
    "\n",
    "\n",
    "save_created_objects()\n",
    "\n",
    "### Data Cleansing and Normalisation Pipeline ###\n",
    "def preprocess_review_text():\n",
    "    # convert the text to lower-case\n",
    "    vodafone_reviews['lower_text'] = vodafone_reviews.text.str.lower()\n",
    "\n",
    "    # correct curly apostrophes\n",
    "    vodafone_reviews.lower_text = vodafone_reviews.lower_text.str.replace(\"’\", \"'\", regex=False)\n",
    "\n",
    "    # correct encoding errors\n",
    "    vodafone_reviews.lower_text = vodafone_reviews.lower_text.str.replace(\"â€™\", \"'\", regex=False)\n",
    "    vodafone_reviews.lower_text = vodafone_reviews.lower_text.str.replace(\"â€“\", \" \", regex=False)\n",
    "    vodafone_reviews.lower_text = vodafone_reviews.lower_text.str.replace(\"\\r\", \" \", regex=False)\n",
    "    vodafone_reviews.lower_text = vodafone_reviews.lower_text.str.replace(\"\\n\", \" \", regex=False)\n",
    "\n",
    "    # create a dictionary of common expansions in the english language\n",
    "    contractions_dict = {\"can't\": \"can not\",\n",
    "                         \"won't\": \"will not\",\n",
    "                         \"don't\": \"do not\",\n",
    "                         \"n't\": \" not\",\n",
    "                         \"'m\": \" am\",\n",
    "                         \"'ll\": \" will\",\n",
    "                         \"'d\": \" would\",\n",
    "                         \"'ve\": \" have\",\n",
    "                         \"'re\": \" are\",\n",
    "                         \"'s\": \"\"} # 's could be 'is' or could be possessive: it has no expansion\n",
    "\n",
    "    # expand the contractions and add to dataframe as new variable\n",
    "    exp_text = []\n",
    "    for review in vodafone_reviews.lower_text:\n",
    "        text = []\n",
    "        for key, value in contractions_dict.items():\n",
    "            if key in review:\n",
    "                review = review.replace(key, value)\n",
    "                text.append(review)\n",
    "        exp_text.append(review)\n",
    "\n",
    "    vodafone_reviews['clean_text'] = exp_text\n",
    "\n",
    "    # remove punctuation and clean up the extra white space between words\n",
    "    vodafone_reviews.clean_text = vodafone_reviews.clean_text.str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "    vodafone_reviews.clean_text = vodafone_reviews.clean_text.apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "    # create a vocabulary from the clean_text column\n",
    "    def list_of_words(df, column):\n",
    "        vocabulary = pd.DataFrame(columns=[\"words\"])\n",
    "        for i in range(len(df)):\n",
    "            words = df[column].iloc[i]\n",
    "            words = words.split(\" \")\n",
    "            vocabulary = vocabulary.append(pd.DataFrame(words, columns=[\"words\"]))\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    vocabulary = list_of_words(vodafone_reviews, \"clean_text\")\n",
    "    vocabulary = vocabulary[vocabulary.words != \"\"] # remove empty strings\n",
    "\n",
    "    # initialise a spellchecker and check the unknown words\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    unknown_words = spell.unknown(vocabulary.words.to_list())\n",
    "    unknown_word_counts = vocabulary[vocabulary.words.isin(unknown_words)].value_counts()\n",
    "\n",
    "    return vodafone_reviews, vocabulary, unknown_words, unknown_word_counts\n",
    "\n",
    "\n",
    "def perform_advanced_preprocessing(nlp=nlp_trf):\n",
    "    # extract parts-of-speech and noun phrases\n",
    "    words = []\n",
    "    poss = []\n",
    "    pos_tags = []\n",
    "    ner_types = []\n",
    "    noun_chunks = []\n",
    "    for review in vodafone_reviews.clean_text:\n",
    "        word = []\n",
    "        pos = []\n",
    "        pos_tag = []\n",
    "        ner_type = []\n",
    "        chunk = []\n",
    "        t = nlp(review)\n",
    "        for w in t:\n",
    "            word.append(w.text)\n",
    "            pos.append(w.pos_)\n",
    "            pos_tag.append(w.tag_)\n",
    "            ner_type.append(w.ent_type_)\n",
    "        words.append(word)\n",
    "        poss.append(pos)\n",
    "        pos_tags.append(pos_tag)\n",
    "        ner_types.append(ner_type)\n",
    "\n",
    "        for c in t.noun_chunks:\n",
    "            chunk.append(c.text)\n",
    "        noun_chunks.append(chunk)\n",
    "\n",
    "    vodafone_reviews['words'] = words\n",
    "    vodafone_reviews['pos'] = poss\n",
    "    vodafone_reviews['pos_tags'] = pos_tags\n",
    "    vodafone_reviews['ner_types'] = ner_types\n",
    "    vodafone_reviews['noun_phrases'] = noun_chunks\n",
    "\n",
    "    # pull out named entities (organisations, money, dates etc)\n",
    "    ent_texts = []\n",
    "    ent_labels = []\n",
    "    for review in nlp.pipe(vodafone_reviews.clean_text):\n",
    "        ent_text = []\n",
    "        ent_label = []\n",
    "        for ent in review.ents:\n",
    "            ent_text.append(ent.text)\n",
    "            ent_label.append(ent.label_)\n",
    "        ent_texts.append(ent_text)\n",
    "        ent_labels.append(ent_label)\n",
    "\n",
    "    vodafone_reviews['ent_text'] = ent_texts\n",
    "    vodafone_reviews['ent_label'] = ent_labels\n",
    "\n",
    "    # check out spacy's stopword list and modify as necessary\n",
    "    spacy_stopwords = nlp.Defaults.stop_words # stopwords are the same irrespective of the English language model used\n",
    "\n",
    "    # lemmatise all words (removing stopwords, punctuation, white space and numbers in the process)\n",
    "    # note that pos needs stopwords etc to provide context, so the pos and pos_tag columns should be treated with\n",
    "    #  caution\n",
    "    preproc_reviews = []\n",
    "    preproc_poss = []\n",
    "    preproc_pos_tags = []\n",
    "    for review in vodafone_reviews.clean_text:\n",
    "        reviews = []\n",
    "        pos = []\n",
    "        pos_tag = []\n",
    "        t = nlp(review)\n",
    "        for w in t:\n",
    "            if not w.is_stop and not w.is_punct and not w.is_digit and not w.is_space:\n",
    "                reviews.append(w.lemma_)\n",
    "                pos.append(w.pos_)\n",
    "                pos_tag.append(w.tag_)\n",
    "        preproc_reviews.append(reviews)\n",
    "        preproc_poss.append(pos)\n",
    "        preproc_pos_tags.append(pos_tag)\n",
    "\n",
    "    vodafone_reviews['preproc_text'] = preproc_reviews\n",
    "    vodafone_reviews['preproc_text_pos'] = preproc_poss\n",
    "    vodafone_reviews['preproc_text_pos_tag'] = preproc_pos_tags\n",
    "\n",
    "    # add probable bigrams and trigrams\n",
    "    bigram_model = gensim.models.Phrases(preproc_reviews)\n",
    "    bigrams = [bigram_model[review] for review in preproc_reviews]\n",
    "\n",
    "    trigram_model = gensim.models.Phrases(bigrams)\n",
    "    trigrams = [trigram_model[review] for review in bigrams]\n",
    "\n",
    "    vodafone_reviews['preproc_bigrams'] = bigrams\n",
    "    vodafone_reviews['preproc_trigrams'] = trigrams\n",
    "\n",
    "    return vodafone_reviews\n",
    "\n",
    "\n",
    "# run both pre-processing functions\n",
    "vodafone_reviews, vocabulary, unknown_words, unknown_word_counts = preprocess_review_text()\n",
    "\n",
    "# caution, this function can take a little while to run, the last execution took 17 minutes\n",
    "start_time = time.time()\n",
    "perform_advanced_preprocessing() # specifying an output isn't necessary in this case, don't ask me why\n",
    "execution_time = time.time() - start_time\n",
    "print('Execution time in minutes: ' + str(execution_time/60))\n",
    "\n",
    "def process_noun_phrases(nlp=nlp_trf):\n",
    "    # place 'noun_phrases' at the end\n",
    "    col_name = \"noun_phrases\"\n",
    "    last_col = vodafone_reviews.pop(col_name)\n",
    "    vodafone_reviews.insert(21, col_name, last_col)\n",
    "\n",
    "    # convert the phrases back into strings\n",
    "    vodafone_reviews['noun_phrase_text'] = vodafone_reviews.noun_phrases.apply(lambda x: '.'.join(x) if x != '' else x)\n",
    "    vodafone_reviews.noun_phrase_text = vodafone_reviews.noun_phrase_text.str.replace(\" \", \"_\", regex=False)\n",
    "    vodafone_reviews.noun_phrase_text = vodafone_reviews.noun_phrase_text.str.replace(\".\", \" \", regex=False)\n",
    "\n",
    "    lemma_phrases = []\n",
    "    preproc_phrases = []\n",
    "    for review in vodafone_reviews.noun_phrase_text:\n",
    "        lemma_reviews = []\n",
    "        reviews = []\n",
    "        t = nlp(review)\n",
    "        for w in t:\n",
    "            if not w.is_stop and not w.is_punct and not w.is_digit and not w.is_space:\n",
    "                lemma_reviews.append(w.lemma_)\n",
    "                reviews.append(w.text)\n",
    "        lemma_phrases.append(lemma_reviews)\n",
    "        preproc_phrases.append(reviews)\n",
    "\n",
    "    vodafone_reviews['preproc_phrases'] = preproc_phrases\n",
    "    vodafone_reviews['preproc_lemma_phrases'] = lemma_phrases\n",
    "\n",
    "    return vodafone_reviews\n",
    "\n",
    "# caution, this function can take a little while to run, the last execution took 4 minutes\n",
    "start_time = time.time()\n",
    "process_noun_phrases()\n",
    "execution_time = time.time() - start_time\n",
    "print('Execution time in minutes: ' + str(execution_time/60))\n",
    "\n",
    "\n",
    "def save_created_objects2():\n",
    "    object_names = [vocabulary, unknown_words, unknown_word_counts, vodafone_reviews]\n",
    "    vocabulary.to_csv('part2_vocabulary.csv', header=True)\n",
    "    unknown_word_counts.to_csv('part2_unknown_word_counts.csv', header=True)\n",
    "\n",
    "    with open('part2_unknown_words', 'wb') as outfile:\n",
    "        pickle.dump(unknown_words, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part2_vodafone_reviews', 'wb') as outfile:\n",
    "        pickle.dump(vodafone_reviews, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "save_created_objects2()\n",
    "\n",
    "### More Advanced EDA ###\n",
    "def create_review_corpora():\n",
    "    review_corpus = vodafone_reviews.loc[:, ['score', 'nps_group', 'preproc_text',\n",
    "                                             'preproc_bigrams', 'preproc_trigrams', 'preproc_lemma_phrases']]\n",
    "    review_corpus['text_strings'] = review_corpus.preproc_text.apply(lambda x: ' '.join(x) if x != '' else x)\n",
    "    review_corpus['bigram_strings'] = review_corpus.preproc_bigrams.apply(lambda x: ' '.join(x) if x != '' else x)\n",
    "    review_corpus['trigram_strings'] = review_corpus.preproc_trigrams.apply(lambda x: ' '.join(x) if x != '' else x)\n",
    "    review_corpus['noun_strings'] = review_corpus.preproc_lemma_phrases.apply(lambda x: ' '.join(x) if x != '' else x)\n",
    "\n",
    "    promoter_corpus = review_corpus[review_corpus.nps_group == 'Promoter']\n",
    "    passive_corpus = review_corpus[review_corpus.nps_group == 'Passive']\n",
    "    detractor_corpus = review_corpus[review_corpus.nps_group == 'Detractor']\n",
    "\n",
    "    return review_corpus, promoter_corpus, passive_corpus, detractor_corpus\n",
    "\n",
    "\n",
    "def generate_wordcloud_text(col_name='noun_strings'):\n",
    "    all_text = ' '.join('' if pd.isna(review) else review for review in review_corpus[col_name])\n",
    "    promoter_text = ' '.join('' if pd.isna(review) else review for review in promoter_corpus[col_name])\n",
    "    passive_text = ' '.join('' if pd.isna(review) else review for review in passive_corpus[col_name])\n",
    "    detractor_text = ' '.join('' if pd.isna(review) else review for review in detractor_corpus[col_name])\n",
    "\n",
    "    return all_text, promoter_text, passive_text, detractor_text\n",
    "\n",
    "\n",
    "review_corpus, promoter_corpus, passive_corpus, detractor_corpus = create_review_corpora()\n",
    "all_text, promoter_text, passive_text, detractor_text = generate_wordcloud_text()\n",
    "\n",
    "# overall word cloud\n",
    "wc_stopwords = ['vodafone', 'vodaphone']\n",
    "wordcloud_reviews = WordCloud(max_font_size=30, max_words=100000, random_state=2021, scale=2, background_color='white',\n",
    "                              contour_width=3, stopwords=set(wc_stopwords), colormap='inferno').generate(all_text)\n",
    "plt.imshow(wordcloud_reviews, interpolation='bilinear')\n",
    "plt.axis(\"off\");\n",
    "\n",
    "# promoter word cloud\n",
    "wc_stopwords = ['vodafone']\n",
    "\n",
    "wordcloud_promoter_reviews = WordCloud(max_font_size=30, max_words=100000, random_state=2021, scale=2,\n",
    "                                       background_color='white', contour_width=3, stopwords=set(wc_stopwords),\n",
    "                                       colormap='winter').generate(promoter_text)\n",
    "plt.imshow(wordcloud_promoter_reviews, interpolation='bilinear')\n",
    "plt.axis(\"off\");\n",
    "\n",
    "# passive word cloud\n",
    "wc_stopwords = ['vodafone']\n",
    "\n",
    "wordcloud_passive_reviews = WordCloud(max_font_size=30, max_words=100000, random_state=2021, scale=2,\n",
    "                                      background_color='white', contour_width=3, stopwords=set(wc_stopwords),\n",
    "                                      colormap='summer').generate(passive_text)\n",
    "plt.imshow(wordcloud_passive_reviews, interpolation='bilinear')\n",
    "plt.axis(\"off\");\n",
    "\n",
    "# detractor word cloud\n",
    "wc_stopwords = ['vodafone']\n",
    "\n",
    "wordcloud_detractor_reviews = WordCloud(max_font_size=30, max_words=100000, random_state=2021, scale=2,\n",
    "                                        background_color='white', contour_width=3, stopwords=set(wc_stopwords),\n",
    "                                        colormap='inferno').generate(detractor_text)\n",
    "plt.imshow(wordcloud_detractor_reviews, interpolation='bilinear')\n",
    "plt.axis(\"off\");\n",
    "\n",
    "# check associated word/phrase frequencies\n",
    "def get_top_n_phrases(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words=['vodafone']).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "\n",
    "all_phrase_counts = get_top_n_phrases(review_corpus.noun_strings, 30)\n",
    "all_phrase_counts_df = pd.DataFrame(all_phrase_counts, columns=['phrase', 'count'])\n",
    "\n",
    "promoter_phrase_counts = get_top_n_phrases(promoter_corpus.noun_strings, 30)\n",
    "promoter_phrase_counts_df = pd.DataFrame(promoter_phrase_counts, columns=['phrase', 'count'])\n",
    "\n",
    "passive_phrase_counts = get_top_n_phrases(passive_corpus.noun_strings, 30)\n",
    "passive_phrase_counts_df = pd.DataFrame(passive_phrase_counts, columns=['phrase', 'count'])\n",
    "\n",
    "detractor_phrase_counts = get_top_n_phrases(detractor_corpus.noun_strings, 30)\n",
    "detractor_phrase_counts_df = pd.DataFrame(detractor_phrase_counts, columns=['phrase', 'count'])\n",
    "\n",
    "p = sns.barplot(x='phrase', y='count', data=all_phrase_counts_df, palette = 'rocket')\n",
    "p.spines['right'].set_visible(False)\n",
    "p.spines['top'].set_visible(False)\n",
    "p.set_xlabel(\"Noun phrase\")\n",
    "p.set_ylabel(\"Frequency\")\n",
    "p.set_xticklabels(all_phrase_counts_df.phrase, rotation=45, ha='right');\n",
    "\n",
    "p2 = sns.barplot(x='phrase', y='count', data=promoter_phrase_counts_df, palette = 'rocket')\n",
    "p2.spines['right'].set_visible(False)\n",
    "p2.spines['top'].set_visible(False)\n",
    "p2.set_xlabel(\"Noun phrase\")\n",
    "p2.set_ylabel(\"Frequency\")\n",
    "p2.set_xticklabels(promoter_phrase_counts_df.phrase, rotation=45, ha='right');\n",
    "\n",
    "p3 = sns.barplot(x='phrase', y='count', data=detractor_phrase_counts_df, palette = 'rocket')\n",
    "p3.spines['right'].set_visible(False)\n",
    "p3.spines['top'].set_visible(False)\n",
    "p3.set_xlabel(\"Noun phrase\")\n",
    "p3.set_ylabel(\"Frequency\")\n",
    "p3.set_xticklabels(detractor_phrase_counts_df.phrase, rotation=45, ha='right');\n",
    "\n",
    "p = sns.barplot(x='phrase', y='count', data=passive_phrase_counts_df, palette = 'rocket')\n",
    "p.spines['right'].set_visible(False)\n",
    "p.spines['top'].set_visible(False)\n",
    "p.set_xlabel(\"Noun phrase\")\n",
    "p.set_ylabel(\"Frequency\")\n",
    "p.set_xticklabels(passive_phrase_counts_df.phrase, rotation=45, ha='right');\n",
    "\n",
    "# create new dataframe and calculate polarity and subjectivity\n",
    "sentiment_df = vodafone_reviews.loc[:, ['score', 'nps_group', 'text', 'clean_text']]\n",
    "sentiment_df['text_strings'] = review_corpus.loc[:, ['text_strings']]\n",
    "\n",
    "sentiment_df['text_polarity'] = sentiment_df.text.map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "sentiment_df['text_subjectivity'] = sentiment_df.text.map(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "sentiment_df['clean_polarity'] = sentiment_df.clean_text.map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "sentiment_df['clean_subjectivity'] = sentiment_df.clean_text.map(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "p = sns.boxplot(x='score', y='text_polarity', data=sentiment_df, palette='flare')\n",
    "p.spines['right'].set_visible(False)\n",
    "p.spines['top'].set_visible(False)\n",
    "p.set_xlabel(\"Customer rating\")\n",
    "p.set_ylabel(\"Polarity\");\n",
    "\n",
    "p = sns.stripplot(x='score', y='text_polarity', data=sentiment_df, palette='flare')\n",
    "p.spines['right'].set_visible(False)\n",
    "p.spines['top'].set_visible(False)\n",
    "p.set_xlabel(\"Customer rating\")\n",
    "p.set_ylabel(\"Polarity\");\n",
    "\n",
    "polarity_stats = sentiment_df.groupby('score')['text_polarity'].agg([np.mean, np.std, np.min, np.max, np.median])\n",
    "\n",
    "\n",
    "def save_created_objects3():\n",
    "    object_names = [review_corpus, promoter_corpus, passive_corpus, detractor_corpus,\n",
    "                    all_text, promoter_text, passive_text, detractor_text,\n",
    "                    all_phrase_counts_df, promoter_phrase_counts_df, passive_phrase_counts_df, detractor_phrase_counts_df,\n",
    "                    vodafone_reviews,\n",
    "                    sentiment_df, polarity_stats]\n",
    "\n",
    "    all_phrase_counts_df.to_csv('part3_all_phrase_counts_df', header=True)\n",
    "    promoter_phrase_counts_df.to_csv('part3_promoter_phrase_counts_df', header=True)\n",
    "    passive_phrase_counts_df.to_csv('part3_passive_phrase_counts_df', header=True)\n",
    "    detractor_phrase_counts_df.to_csv('part3_detractor_phrase_counts_df', header=True)\n",
    "    sentiment_df.to_csv('part3_sentiment_df', header=True)\n",
    "    polarity_stats.to_csv('part3_polarity_stats', header=True)\n",
    "\n",
    "    with open('part3_review_corpus', 'wb') as outfile:\n",
    "        pickle.dump(review_corpus, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_promoter_corpus', 'wb') as outfile:\n",
    "        pickle.dump(promoter_corpus, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_passive_corpus', 'wb') as outfile:\n",
    "        pickle.dump(passive_corpus, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_detractor_corpus', 'wb') as outfile:\n",
    "        pickle.dump(detractor_corpus, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_all_text', 'wb') as outfile:\n",
    "        pickle.dump(all_text, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_promoter_text', 'wb') as outfile:\n",
    "        pickle.dump(promoter_text, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_passive_text', 'wb') as outfile:\n",
    "        pickle.dump(passive_text, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_detractor_text', 'wb') as outfile:\n",
    "        pickle.dump(detractor_text, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    with open('part3_vodafone_reviews', 'wb') as outfile:\n",
    "        pickle.dump(vodafone_reviews, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "save_created_objects3()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "project1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
